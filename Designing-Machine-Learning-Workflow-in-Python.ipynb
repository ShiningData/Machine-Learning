{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing Machine Learning Workflows in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Standard Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv('credit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checking_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>employment</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>other_parties</th>\n",
       "      <th>...</th>\n",
       "      <th>property_magnitude</th>\n",
       "      <th>age</th>\n",
       "      <th>other_payment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>own_telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'&lt;0'</td>\n",
       "      <td>6</td>\n",
       "      <td>'critical/other existing credit'</td>\n",
       "      <td>buy_radio_tv</td>\n",
       "      <td>1169</td>\n",
       "      <td>'no known savings'</td>\n",
       "      <td>'&gt;=7'</td>\n",
       "      <td>4</td>\n",
       "      <td>'male single'</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>'real estate'</td>\n",
       "      <td>67</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>2</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'0&lt;=X&lt;200'</td>\n",
       "      <td>48</td>\n",
       "      <td>'existing paid'</td>\n",
       "      <td>buy_radio_tv</td>\n",
       "      <td>5951</td>\n",
       "      <td>'&lt;100'</td>\n",
       "      <td>'1&lt;=X&lt;4'</td>\n",
       "      <td>2</td>\n",
       "      <td>'female div/dep/mar'</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>'real estate'</td>\n",
       "      <td>22</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'no checking'</td>\n",
       "      <td>12</td>\n",
       "      <td>'critical/other existing credit'</td>\n",
       "      <td>education</td>\n",
       "      <td>2096</td>\n",
       "      <td>'&lt;100'</td>\n",
       "      <td>'4&lt;=X&lt;7'</td>\n",
       "      <td>2</td>\n",
       "      <td>'male single'</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>'real estate'</td>\n",
       "      <td>49</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1</td>\n",
       "      <td>'unskilled resident'</td>\n",
       "      <td>2</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  checking_status  duration                    credit_history       purpose  \\\n",
       "0            '<0'         6  'critical/other existing credit'  buy_radio_tv   \n",
       "1      '0<=X<200'        48                   'existing paid'  buy_radio_tv   \n",
       "2   'no checking'        12  'critical/other existing credit'     education   \n",
       "\n",
       "   credit_amount      savings_status employment  installment_commitment  \\\n",
       "0           1169  'no known savings'      '>=7'                       4   \n",
       "1           5951              '<100'   '1<=X<4'                       2   \n",
       "2           2096              '<100'   '4<=X<7'                       2   \n",
       "\n",
       "        personal_status other_parties  ...   property_magnitude age  \\\n",
       "0         'male single'          none  ...        'real estate'  67   \n",
       "1  'female div/dep/mar'          none  ...        'real estate'  22   \n",
       "2         'male single'          none  ...        'real estate'  49   \n",
       "\n",
       "   other_payment_plans housing existing_credits                   job  \\\n",
       "0                 none     own                2               skilled   \n",
       "1                 none     own                1               skilled   \n",
       "2                 none     own                1  'unskilled resident'   \n",
       "\n",
       "  num_dependents  own_telephone foreign_worker class  \n",
       "0              1            yes            yes  good  \n",
       "1              1           none            yes   bad  \n",
       "2              2           none            yes  good  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first few lines of your data using head()\n",
    "credit.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all columns except float based \n",
    "non_numeric_columns = [col for col in credit.select_dtypes(exclude ='int64').columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a label encoder for each column. Encode the values\n",
    "for column in non_numeric_columns:\n",
    "    le = LabelEncoder()\n",
    "    credit[column] = le.fit_transform(credit[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking_status           int32\n",
      "duration                  int64\n",
      "credit_history            int32\n",
      "purpose                   int32\n",
      "credit_amount             int64\n",
      "savings_status            int32\n",
      "employment                int32\n",
      "installment_commitment    int64\n",
      "personal_status           int32\n",
      "other_parties             int32\n",
      "residence_since           int64\n",
      "property_magnitude        int32\n",
      "age                       int64\n",
      "other_payment_plans       int32\n",
      "housing                   int32\n",
      "existing_credits          int64\n",
      "job                       int32\n",
      "num_dependents            int64\n",
      "own_telephone             int32\n",
      "foreign_worker            int32\n",
      "class                     int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data types of the columns of the data frame\n",
    "print(credit.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = credit.drop(['class'], 1), credit['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test, with 20% as test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "  X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create a random forest classifier, fixing the seed to 2\n",
    "rf_model = RandomForestClassifier(random_state=2).fit(\n",
    "  X_train, y_train)\n",
    "\n",
    "# Use it to predict the labels of the test data\n",
    "rf_predictions = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracies = {}\n",
    "accuracies['rf'] = accuracy_score(y_test, rf_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#np.mean(cross_val_score(RandomForestClassifier(), X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 40}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set a range for n_estimators from 10 to 40 in steps of 10\n",
    "param_grid = {'n_estimators': range(10, 50, 10)}\n",
    "\n",
    "# Optimize for a RandomForestClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 10}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Define a grid for n_estimators ranging from 1 to 10\n",
    "param_grid = {'n_estimators': range(1, 11)}\n",
    "\n",
    "# Optimize for a AdaBoostClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(AdaBoostClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 50}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define a grid for n_neighbors with values 10, 50 and 100\n",
    "param_grid = {'n_neighbors': [10, 50, 100]}\n",
    "\n",
    "# Optimize for KNeighborsClassifier() using GridSearchCV\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3)\n",
    "grid.fit(X, y)\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Encodings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Create numeric encoding for credit_history\n",
    "credit_history_num = LabelEncoder().fit_transform(\n",
    "  credit['credit_history'])\n",
    "\n",
    "# Create a new feature matrix including the numeric encoding\n",
    "X_num = pd.concat([X, pd.Series(credit_history_num)], 1)\n",
    "\n",
    "# Create new feature matrix with dummies for credit_history\n",
    "X_hot = pd.concat(\n",
    "  [X, pd.get_dummies(credit['credit_history'])], 1)\n",
    "\n",
    "# Compare the number of features of the resulting DataFrames\n",
    "print(X_hot.shape[1] > X_num.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are discussing the credit dataset with the bank manager. She suggests that the safest loan applications tend to request mid-range credit amounts. Values that are either too low or too high suggest high risk. This means that a non-linear relationship might exist between this variable and the class. You want to test this hypothesis. You will construct a non-linear transformation of the feature. Then, you will assess which of the two features is better at predicting the class using SelectKBest() and the chi2() metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function computing absolute difference from column mean\n",
    "def abs_diff(x):\n",
    "    return np.abs(x-np.mean(x))\n",
    "\n",
    "# Apply it to the credit amount and store to new column\n",
    "credit['diff'] = abs_diff(credit['credit_amount'])\n",
    "\n",
    "# Create a feature selector with chi2 that picks one feature\n",
    "sk = SelectKBest(chi2, k=1)\n",
    "\n",
    "# Use the selector to pick between credit_amount and diff\n",
    "sk.fit(credit[['credit_amount', 'diff']], credit['class'])\n",
    "\n",
    "# Inspect the results\n",
    "sk.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bringing it all together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = pd.read_csv('arrh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>QRSduration</th>\n",
       "      <th>PRinterval</th>\n",
       "      <th>Q-Tinterval</th>\n",
       "      <th>Tinterval</th>\n",
       "      <th>Pinterval</th>\n",
       "      <th>QRS</th>\n",
       "      <th>...</th>\n",
       "      <th>chV6_QwaveAmp</th>\n",
       "      <th>chV6_RwaveAmp</th>\n",
       "      <th>chV6_SwaveAmp</th>\n",
       "      <th>chV6_RPwaveAmp</th>\n",
       "      <th>chV6_SPwaveAmp</th>\n",
       "      <th>chV6_PwaveAmp</th>\n",
       "      <th>chV6_TwaveAmp</th>\n",
       "      <th>chV6_QRSA</th>\n",
       "      <th>chV6_QRSTA</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>193</td>\n",
       "      <td>371</td>\n",
       "      <td>174</td>\n",
       "      <td>121</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>174</td>\n",
       "      <td>401</td>\n",
       "      <td>149</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>95</td>\n",
       "      <td>138</td>\n",
       "      <td>163</td>\n",
       "      <td>386</td>\n",
       "      <td>185</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>380</td>\n",
       "      <td>179</td>\n",
       "      <td>143</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>181</td>\n",
       "      <td>360</td>\n",
       "      <td>177</td>\n",
       "      <td>103</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  height  weight  QRSduration  PRinterval  Q-Tinterval  Tinterval  \\\n",
       "0   75    0     190      80           91         193          371        174   \n",
       "1   56    1     165      64           81         174          401        149   \n",
       "2   54    0     172      95          138         163          386        185   \n",
       "3   55    0     175      94          100         202          380        179   \n",
       "4   75    0     190      80           88         181          360        177   \n",
       "\n",
       "   Pinterval  QRS  ...    chV6_QwaveAmp  chV6_RwaveAmp  chV6_SwaveAmp  \\\n",
       "0        121  -16  ...              0.0            9.0           -0.9   \n",
       "1         39   25  ...              0.0            8.5            0.0   \n",
       "2        102   96  ...              0.0            9.5           -2.4   \n",
       "3        143   28  ...              0.0           12.2           -2.2   \n",
       "4        103  -16  ...              0.0           13.1           -3.6   \n",
       "\n",
       "   chV6_RPwaveAmp  chV6_SPwaveAmp  chV6_PwaveAmp  chV6_TwaveAmp  chV6_QRSA  \\\n",
       "0             0.0             0.0            0.9            2.9       23.3   \n",
       "1             0.0             0.0            0.2            2.1       20.4   \n",
       "2             0.0             0.0            0.3            3.4       12.3   \n",
       "3             0.0             0.0            0.4            2.6       34.6   \n",
       "4             0.0             0.0           -0.1            3.9       25.4   \n",
       "\n",
       "   chV6_QRSTA  class  \n",
       "0        49.4      0  \n",
       "1        38.8      0  \n",
       "2        49.0      0  \n",
       "3        61.6      1  \n",
       "4        62.8      0  \n",
       "\n",
       "[5 rows x 280 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452, 280)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "# Find from sklearn.ensemble import RandomForestClassifierhe best value for max_depth among values 2, 5 and 10\n",
    "grid_search = GridSearchCV(\n",
    "  RandomForestClassifier(random_state=1), param_grid={'max_depth': [2, 5, 10]})\n",
    "\n",
    "best_value = grid_search.fit(\n",
    "  X_train, y_train).best_params_['max_depth']\n",
    "\n",
    "# Using the best value from above, fit a random forest\n",
    "clf = RandomForestClassifier(\n",
    "  random_state=1, max_depth=best_value).fit(X_train, y_train)\n",
    "\n",
    "# Apply SelectKBest with chi2 and pick top 100 features\n",
    "vt = SelectKBest(chi2, k=20).fit(X_train, y_train)\n",
    "\n",
    "# Create a new dataset only containing the selected features\n",
    "X_train_reduced = vt.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 20)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Human in the Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = pd.read_csv('lanl_flows.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['time', 'duration', 'source_computer', 'source_port',\n",
       "       'destination_computer', 'destination_port', 'protocol',\n",
       "       'packet_count', 'byte_count'], dtype=object)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit of analysis = destination_computer\n",
    "\n",
    "flows_grouped = flows.groupby('destination_computer')\n",
    "\n",
    "# Access one of its elements you have to call list()\n",
    "list(flows_grouped)[0]\n",
    "\n",
    "# Convert each of these DataFrames to feature vectors.\n",
    "def featurize(df):\n",
    "    return {\n",
    "        'unique_ports': len(set(df['destination_port'])),\n",
    "        'average_packet': np.mean(df['packet_count']),\n",
    "        'average_duration': np.mean(df['duration'])\n",
    "    }\n",
    "\n",
    "# Applying this function on each group yields an iterator containing one vector per group, all of the same length. \n",
    "out = flows.groupby('destination_computer').apply(featurize)\n",
    "\n",
    "# Create DF one row for destination computer\n",
    "X = pd.DataFrame(list(out), index=out.index)\n",
    "X.head()\n",
    "\n",
    "bads = set(attacks['source_computer'].append(attacks['destination_computer']))\n",
    "y = [x in bads for x in X.index]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "clf = AdaBoostClassifier()\n",
    "accuracy_score(y_test, clf.fit(X_train, y_train).predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is the source or the destination bad?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lesson, you used the destination computer as your entity of interest. However, your cybersecurity analyst just told you that it is the infected machines that generate the bad traffic, and will therefore appear as a source, not a destination, in the flows dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each of these DataFrames to feature vectors.\n",
    "def featurize(df):\n",
    "    return {\n",
    "        'unique_ports': len(set(df['destination_port'])),\n",
    "        'average_packet': np.mean(df['packet_count']),\n",
    "        'average_duration': np.mean(df['duration'])\n",
    "    }\n",
    "\n",
    "# Group by source computer, and apply the feature extractor \n",
    "out = flows.groupby('source_computer').apply(featurize)\n",
    "\n",
    "# Convert the iterator to a dataframe by calling list on it\n",
    "X = pd.DataFrame(list(out), index=out.index)\n",
    "\n",
    "bads = set(attacks['source_computer'].append(attacks['destination_computer']))\n",
    "y = [x in bads for x in X.index]\n",
    "\n",
    "# Check which sources in X.index are bad to create labels\n",
    "y = [x in bads for x in X.index]\n",
    "\n",
    "# Report the average accuracy of Adaboost over 3-fold CV\n",
    "print(np.mean(cross_val_score(AdaBoostClassifier(), X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature counting unique protocols per source\n",
    "protocols = flows.groupby('source_computer').apply(\n",
    "  lambda df: len(set(df['protocol'])))\n",
    "\n",
    "# Convert this feature into a dataframe, naming the column\n",
    "protocols_DF = pd.DataFrame(\n",
    "  protocols, index=protocols.index, columns=['protocol'])\n",
    "\n",
    "# Now concatenate this feature with the previous dataset, X\n",
    "X_more = pd.concat([X, protocols_DF], axis=1)\n",
    "\n",
    "# Refit the classifier and report its accuracy\n",
    "print(np.mean(cross_val_score(\n",
    "  AdaBoostClassifier(), X_more, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**imperfect labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From features to labels\n",
    "\n",
    "# Convert a feature into a labeling heuristic:\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "y_weak_train = X_train['unique_ports'] > 15\n",
    "\n",
    "X_train_aug = pd.concat([X_train, X_train])\n",
    "y_train_aug = pd.concat([pd.Series(y_train), pd.Series(y_weak_train)])\n",
    "\n",
    "weights = [1.0]*len(y_train) + [0.1]*len(y_weak_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning a heuristic into a classifier\n",
    " \n",
    "You are surprised by the fact that heuristics can be so helpful. So you decide to treat the heuristic that \"too many unique ports is suspicious\" as a classifier in its own right. You achieve that by thresholding the number of unique ports per source by the average number used in bad source computers -- these are computers for which the label is True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataset X_train_bad by subselecting bad hosts\n",
    "X_train_bad = X_train[y_train]\n",
    "\n",
    "# Calculate the average of unique_ports in bad examples\n",
    "avg_bad_ports = np.mean(X_train_bad['unique_ports'])\n",
    "\n",
    "# Label as positive sources that use more ports than that\n",
    "pred_port = X_test['unique_ports'] > avg_bad_ports\n",
    "\n",
    "# Print the accuracy of the heuristic\n",
    "print(accuracy_score(y_test, pred_port))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining heuristics\n",
    "\n",
    "A different cyber analyst tells you that during certain types of attack, the infected source computer sends small bits of traffic, to avoid detection. This makes you wonder whether it would be better to create a combined heuristic that simultaneously looks for large numbers of ports and small packet sizes. Does this improve performance over the simple port heuristic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean of average_packet for bad sources\n",
    "avg_bad_packet = np.mean(X_train[y_train]['average_packet'])\n",
    "\n",
    "# Label as positive if average_packet is lower than that\n",
    "pred_packet = X_test['average_packet'] < avg_bad_packet\n",
    "\n",
    "# Find indices where pred_port and pred_packet both True\n",
    "pred_port = X_test['unique_ports'] > avg_bad_ports\n",
    "pred_both = pred_packet * pred_port\n",
    "\n",
    "# Ports only produced an accuracy of 0.919. Is this better?\n",
    "print(accuracy_score(y_test, pred_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with label noise\n",
    "\n",
    "One of your cyber analysts informs you that many of the labels for the first 100 source computers in your training data might be wrong because of a database error. She hopes you can still use the data because most of the labels are still correct, but asks you to treat these 100 labels as \"noisy\". Thankfully you know how to do that, using weighted learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Gaussian Naive Bayes classifier to the training data\n",
    "clf = GaussianNB().fit(X_train, y_train_noisy)\n",
    "\n",
    "# Report its accuracy on the test data\n",
    "print(accuracy_score(y_test, clf.predict(X_test)))\n",
    "\n",
    "# Assign half the weight to the first 100 noisy examples\n",
    "weights = [0.5]*100 + [1.0]*(len(y_train_noisy)-100)\n",
    "\n",
    "# Refit using weights and report accuracy. Has it improved?\n",
    "clf_weights = GaussianNB().fit(X_train, y_train_noisy, sample_weight=weights)\n",
    "print(accuracy_score(y_test, clf_weights.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Functions Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize label\n",
    "kdd['label'] = kdd['label'] == 'bad'\n",
    "\n",
    "# Fit a Gaussian Naive Bayes classier:\n",
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "results = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'predicted': predictions\n",
    "})\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_mat = confusion_matrix(ground_truth, predictions)\n",
    "tn, fp, fn, tp = conf_mat.ravel()\n",
    "\n",
    "# Scalar performance metrics\n",
    "accuracy = 1-(fp + fn)/len(ground_truth)  # (tp + tn)/len(y_test)\n",
    "recall = tp/(tp+fn)\n",
    "fpr = fp/(tn+fp)\n",
    "precision = tp/(tp+fp)\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "accuracy_score(ground_truth, predictions)\n",
    "recall_score(ground_truth, predictions)\n",
    "precision_score(ground_truth, predictions)\n",
    "f1_score(ground_truth, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world cost analysis\n",
    "\n",
    "You will still work on the credit dataset for this exercise. Recall that a \"positive\" in this dataset means \"bad credit\", i.e., a customer who defaulted on their loan, and a \"negative\" means a customer who continued to pay without problems. The bank manager informed you that the bank makes 10K profit on average from each \"good risk\" customer, but loses 150K from each \"bad risk\" customer. Your algorithm will be used to screen applicants, so those that are labeled as \"negative\" will be given a loan, and the \"positive\" ones will be turned down. What is the total cost of your classifier? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest classifier to the training data\n",
    "clf = RandomForestClassifier(random_state=2).fit(X_train, y_train)\n",
    "\n",
    "# Label the test data\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "# Get false positives/negatives from the confusion matrix\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# Now compute the cost using the manager's advice\n",
    "cost = fp*10 + fn*150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB().fit(X_train, y_train)\n",
    "scores = clf.predict_proba(X_test)\n",
    "\n",
    "[s[1] > 0.5 for s in scores] == clf.predict(X_test)\n",
    "\n",
    "# ROC Curves\n",
    "fpr, tpr, thres = roc_curve(\n",
    "ground_truth,\n",
    "[s[1] for s in scores])\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "# AUC\n",
    "clf = AdaBoostClassifier().fit(X_train, y_train)\n",
    "scores_ab = clf.predict_proba(X_test)\n",
    "roc_auc_score(ground_truth, [s[1] for s in scores_ab])\n",
    "\n",
    "# Cost Minimisation\n",
    "def my_scorer(y_test, y_est, cost_fp=10.0, cost_fn=1.0):\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_est).ravel()\n",
    "return cost_fp*fp + cost_fn*fn\n",
    "\n",
    "t_range = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "costs = [\n",
    "my_scorer(y_test, [s[1] > thres for s in scores]) for thres in t_range\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default thresholding\n",
    "\n",
    "You would like to confirm that the DecisionTreeClassifier() uses the same default classification threshold as mentioned in the previous lesson, namely 0.5. It seems strange to you that all classifiers should use the same threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the test data using the given classifier\n",
    "scores = clf.predict_proba(X_test)\n",
    "\n",
    "# Get labels from the scores using the default threshold\n",
    "preds = [s[1] > 0.5 for s in scores]\n",
    "\n",
    "# Use the predict method to label the test data again\n",
    "preds_default = clf.predict(X_test)\n",
    "\n",
    "# Compare the two sets of predictions\n",
    "all(preds == preds_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing the threshold\n",
    "\n",
    "You heard that the default value of 0.5 maximizes accuracy in theory, but you want to test what happens in practice. So you try out a number of different threshold values, to see what accuracy you get, and hence determine the best-performing threshold value. You repeat this experiment for the F1 score. Is 0.5 the optimal threshold? Is the optimal threshold for accuracy and for the F1 score the same? Go ahead and find out! You have a scores matrix available, obtained by scoring the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a range of equally spaced threshold values\n",
    "t_range = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "# Store the predicted labels for each value of the threshold\n",
    "preds = [[s[1] > thr for s in scores] for thr in t_range]\n",
    "\n",
    "# Compute the accuracy for each threshold\n",
    "accuracies = [accuracy_score(y_test, p) for p in preds]\n",
    "\n",
    "# Compute the F1 score for each threshold\n",
    "f1_scores = [f1_score(y_test, p) for p in preds]\n",
    "\n",
    "# Report the optimal threshold for accuracy, and for F1\n",
    "print(t_range[argmax(accuracies)], t_range[argmax(f1_scores)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all together\n",
    "\n",
    "One of the engineers in your arrhythmia detection startup rushes into your office to let you know that there is a problem with the ECG sensor for overweight users. You decide to reduce the influence of examples with weight over 80 by 50%. You are also told that since your startup is targeting the fitness market and makes no medical claims, scaring an athlete unnecessarily is costlier than missing a possible case of arrhythmia. You decide to create a custom loss that makes each \"false alarm\" ten times costlier than missing a case of arrhythmia. Does down-weighting overweight subjects improve this custom loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scorer assigning more cost to false positives\n",
    "def my_scorer(y_test, y_est, cost_fp=10.0, cost_fn=1.0):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_est).ravel()\n",
    "    return cost_fp*fp + cost_fn*fn\n",
    "\n",
    "# Fit a DecisionTreeClassifier to the data and compute the loss\n",
    "clf = DecisionTreeClassifier(random_state=2).fit(X_train, y_train)\n",
    "print(my_scorer(y_test, clf.predict(X_test)))\n",
    "\n",
    "# Refit, downweighting subjects whose weight is above 80\n",
    "weights = [0.5 if w > 80 else 1.0 for w in X_train.weight]\n",
    "clf_weighted = DecisionTreeClassifier(random_state=2).fit(\n",
    "  X_train, y_train, sample_weight=weights)\n",
    "print(my_scorer(y_test, clf_weighted.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Lifecycle Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Workflows to Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "grid_search = GridSearchCV(rf(), param_grid={'max_depth': [2, 5, 10]})\n",
    "grid_search.fit(X_train, y_train)\n",
    "depth = grid_search.best_params_['max_depth']\n",
    "\n",
    "vt = SelectKBest(f_classif, k=3).fit(X_train, y_train)\n",
    "clf = rf(max_depth=best_value).fit(vt.transform(X_train), y_train)\n",
    "accuracy_score(clf.predict(vt.transform(X_test), y_test))\n",
    "\n",
    "# Optimize max_depth\n",
    "pg = {'max_depth': [2,5,10]}\n",
    "gs = GridSearchCV(rf(),\n",
    "param_grid=pg)\n",
    "gs.fit(X_train, y_train)\n",
    "depth = gs.best_params_['max_depth']\n",
    "\n",
    "# Optimize n_estimators\n",
    "pg = {'n_estimators': [10,20,30]}\n",
    "gs = GridSearchCV(\n",
    "rf(max_depth=depth),\n",
    "param_grid=pg)\n",
    "gs.fit(X_train, y_train)\n",
    "n_est = gs.best_params_[\n",
    "'n_estimators']\n",
    "\n",
    "# Jointly max_depth and n_estimators\n",
    "pg = {\n",
    "'max_depth': [2,5,10],\n",
    "'n_estimators': [10,20,30]\n",
    "}\n",
    "gs = GridSearchCV(rf(),\n",
    "param_grid=pg)\n",
    "gs.fit(X_train, y_train)\n",
    "print(gs.best_params_)\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "('feature_selection'\n",
    ", SelectKBest(f_classif)),\n",
    "('classifier'\n",
    ", RandomForestClassifier())\n",
    "])\n",
    "\n",
    "params = dict(\n",
    "feature_selection__k=[2, 3, 4],\n",
    "classifier__max_depth=[5, 10, 20]\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "gs = grid_search.fit(X_train, y_train).best_params_\n",
    "\n",
    "# Customizing your pipeline\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "auc_scorer = make_scorer(roc_auc_score)\n",
    "grid_search = GridSearchCV(pipe, param_grid=params, scoring=auc_scorer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first pipeline \n",
    "\n",
    "Back in the arrhythmia startup, your monthly review is coming up, and as part of that an expert Python programmer will be reviewing your code. You decide to tidy up by following best practices and replace your script for feature selection and random forest classification, with a pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with feature selector and classifier\n",
    "pipe = Pipeline([\n",
    "    ('feature_selection', SelectKBest(f_classif)),\n",
    "    ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "params = {\n",
    "   'feature_selection__k':[10, 20],\n",
    "   'clf__n_estimators':[2, 5]}\n",
    "\n",
    "# Initialise the grid search object\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)\n",
    "\n",
    "# Fit it to the data and print the best value combination\n",
    "print(grid_search.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom scorers in pipelines\n",
    "\n",
    "You are proud of the improvement in your code quality, but just remembered that previously you had to use a custom scoring metric in order to account for the fact that false positives are costlier to your startup than false negatives. You hence want to equip your pipeline with scorers other than accuracy, including roc_auc_score(), f1_score(), and you own custom scoring function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom scorer\n",
    "scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Initialize the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom scorer\n",
    "scorer = make_scorer(f1_score)\n",
    "\n",
    "# Initialise the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom scorer\n",
    "scorer = make_scorer(my_metric)\n",
    "\n",
    "# Initialise the CV object\n",
    "gs = GridSearchCV(pipe, param_grid=params, scoring=scorer)\n",
    "\n",
    "# Fit it to the data and print the winning combination\n",
    "print(gs.fit(X_train, y_train).best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Deployment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing the model: \n",
    "\n",
    "#Store a classier to file:\n",
    "import pickle\n",
    "clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file=file)\n",
    "\n",
    "# Load it again from file:\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    clf2 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing your pipeline:\n",
    "\n",
    "# Development environment\n",
    "\n",
    "vt = SelectKBest(f_classif).fit(X_train, y_train)\n",
    "clf = RandomForestClassifier().fit(vt.transform(X_train), y_train)\n",
    "\n",
    "with open('vt.pkl', 'wb') as file:\n",
    "    pickle.dump(vt)\n",
    "\n",
    "with open('clf.pkl','wb') as file:\n",
    "    pickle.dump(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing your pipeline:\n",
    "\n",
    "# Production environment\n",
    "\n",
    "with open('vt.pkl', 'rb') as file:\n",
    "    vt = pickle.load(vt)\n",
    "\n",
    "with open('clf.pkl', 'rb') as file:\n",
    "    clf = pickle.load(clf)\n",
    "\n",
    "clf.predict(vt.transform(X_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing your pipeline:\n",
    "\n",
    "# Development environment\n",
    "\n",
    "pipe = Pipeline([('fs', SelectKBest(f_classif)), ('clf', RandomForestClassifier())])\n",
    "params = dict(fs__k=[2, 3, 4],\n",
    "clf__max_depth=[5, 10, 20])\n",
    "gs = GridSearchCV(pipe, params)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "with open('pipe.pkl', 'wb') as file:\n",
    "    pickle.dump(gs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serializing your pipeline:\n",
    "\n",
    "# Production environment\n",
    "\n",
    "with open('pipe.pkl', 'rb') as file:\n",
    "    gs = pickle.dump(gs, file)\n",
    "gs.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom feature transformations\n",
    "def negate_second_column(X):\n",
    "    Z = X.copy()\n",
    "    Z[:,1] = -Z[:,1]\n",
    "    return Z\n",
    "\n",
    "pipe = Pipeline([('ft', FunctionTransformer(negate_second_column)), ('clf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickles\n",
    "\n",
    "Finally, it is time for you to push your first model to production. It is a random forest classifier which you will use as a baseline, while you are still working to develop a better alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a random forest to the training set\n",
    "clf = RandomForestClassifier(random_state=42).fit(\n",
    "  X_train, y_train)\n",
    "\n",
    "# Save it to a file, to be pushed to production\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(clf, file=file)\n",
    "\n",
    "# Now load the model from file in the production environment\n",
    "with open('model.pkl', 'rb') as file:\n",
    "    clf_from_file = pickle.load(file)\n",
    "\n",
    "# Predict the labels of the test dataset\n",
    "preds = clf_from_file.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom function transformers in pipelines\n",
    "\n",
    "At some point, you were told that the sensors might be performing poorly for obese individuals. Previously you had dealt with that using weights, but now you are thinking that this information might also be useful for feature engineering, so you decide to replace the recorded weight of an individual with an indicator of whether they are obese. You want to do this using pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a feature extractor to flag very large values\n",
    "def more_than_average(X, multiplier=1.0):\n",
    "  Z = X.copy()\n",
    "  Z[:,1] = Z[:,1] > multiplier*np.mean(Z[:,1])\n",
    "  return Z\n",
    "\n",
    "# Convert your function so that it can be used in a pipeline\n",
    "pipe = Pipeline([\n",
    "  ('ft', FunctionTransformer(more_than_average)),\n",
    "  ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Optimize the parameter multiplier using GridSearchCV\n",
    "params = {'ft__multiplier':[1, 2, 3]}\n",
    "grid_search = GridSearchCV(pipe, param_grid=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterating without Overfitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation results\n",
    "\n",
    "grid_search = GridSearchCV(pipe, params, cv=3, return_train_score=True)\n",
    "gs = grid_search.fit(X_train, y_train)\n",
    "results = pd.DataFrame(gs.cv_results_)\n",
    "\n",
    "results[['mean_train_score','std_train_score','mean_test_score','std_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge the champion\n",
    "\n",
    "Having pushed your random forest to production, you suddenly worry that a naive Bayes classifier might be better. You want to run a champion-challenger test, by comparing a naive Bayes, acting as the challenger, to exactly the model which is currently in production, which you will load from file to make sure there is no confusion. You will use the F1 score for assessment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the current model from disk\n",
    "champion = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "# Fit a Gaussian Naive Bayes to the training data\n",
    "challenger = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "# Print the F1 test scores of both champion and challenger\n",
    "print(f1_score(y_test, champion.predict(X_test)))\n",
    "print(f1_score(y_test, challenger.predict(X_test)))\n",
    "\n",
    "# Write back to disk the best-performing model\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(champion, file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation statistics\n",
    "\n",
    "You used grid search CV to tune your random forest classifier, and now want to inspect the cross-validation results to ensure you did not overfit. In particular you would like to take the difference of the mean test score for each fold from the mean training score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your pipeline using GridSearchCV with three folds\n",
    "grid_search = GridSearchCV(pipe, params, cv=3, return_train_score=True)\n",
    "\n",
    "# Fit the grid search\n",
    "gs = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Store the results of CV into a pandas dataframe\n",
    "results = pd.DataFrame(gs.cv_results_)\n",
    "\n",
    "# Print the difference between mean test and training scores\n",
    "print(results['mean_test_score']-results['mean_train_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Shift**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "\n",
    "# Sliding Window\n",
    "window = (t_now-window_size+1):t_now\n",
    "sliding_window = elec.loc[window]\n",
    "\n",
    "# Expanding Window\n",
    "window = 0:t_now\n",
    "expanding_window = elec.loc[window]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shift detection\n",
    "\n",
    "# t_now = 40000, window_size = 20000\n",
    "clf_full = RandomForestClassifier().fit(X, y)\n",
    "clf_sliding = RandomForestClassifier().fit(sliding_X, sliding_y)\n",
    "\n",
    "# Use future data as test\n",
    "test = elec.loc[t_now:elec.shape[0]]\n",
    "test_X = test.drop('class', 1); test_y = test['class']\n",
    "\n",
    "roc_auc_score(test_y, clf_full.predict(test_X))\n",
    "roc_auc_score(test_y, clf_sliding.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size\n",
    "for w_size in range(10, 100, 10):\n",
    "    sliding = arrh.loc[\n",
    "    (t_now - w_size + 1):t_now]\n",
    "    X = sliding.drop('class', 1)\n",
    "    y = sliding['class']\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X, y)\n",
    "    preds = clf.predict(test_X)\n",
    "    roc_auc_score(test_y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the window size\n",
    "\n",
    "You want to check for yourself that the optimal window size for the arrhythmia dataset is 50. You have been given the dataset as a pandas data frame called arrh, and want to use a subset of the data up to time t_now.  the possibility of dataset shift introduces yet another parameter to optimize: the window size. This cannot be done with Cross-Validation on historical data, but instead requires the technique shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over window sizes\n",
    "for w_size in wrange:\n",
    "\n",
    "    # Define sliding window\n",
    "    sliding = arrh.loc[(t_now - w_size + 1):t_now]\n",
    "\n",
    "    # Extract X and y from the sliding window\n",
    "    X, y = sliding.drop('class', 1), sliding['class']\n",
    "    \n",
    "    # Fit the classifier and store the F1 score\n",
    "    preds = GaussianNB().fit(X, y).predict(X_test)\n",
    "    accuracies.append(f1_score(y_test, preds))\n",
    "\n",
    "# Estimate the best performing window size\n",
    "optimal_window = wrange[np.argmax(accuracies)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all together\n",
    "\n",
    "You have two concerns about your pipeline at the arrhythmia detection startup:\n",
    "\n",
    "The app was trained on patients of all ages, but is primarily being used by fitness users who tend to be young. You suspect this might be a case of domain shift, and hence want to disregard all examples above 50 years old.\n",
    "You are still concerned about overfitting, so you want to see if making the random forest classifier less complex and selecting some features might help with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline \n",
    "pipe = Pipeline([\n",
    "  ('ft', SelectKBest()), ('clf', RandomForestClassifier(random_state=2))])\n",
    "\n",
    "# Create a parameter grid\n",
    "grid = {'ft__k':[5, 10], 'clf__max_depth':[10, 20]}\n",
    "\n",
    "# Execute grid search CV on a dataset containing under 50s\n",
    "grid_search = GridSearchCV(pipe, param_grid=grid)\n",
    "arrh = arrh.iloc[np.where(arrh['age'] < 50)]\n",
    "grid_search.fit(arrh.drop('class', 1), arrh['class'])\n",
    "\n",
    "# Push the fitted pipeline to production\n",
    "with open('pipe.pkl', 'wb') as file:\n",
    "    pickle.dump(grid_search, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anomaly Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local outlier factor (LoF)\n",
    "from sklearn.neighbors import LocalOutlierFactor as lof\n",
    "clf = lof()\n",
    "y_pred = clf.fit_predict(X)\n",
    "\n",
    "clf.negative_outlier_factor_[:4]\n",
    "\n",
    "confusion_matrix(y_pred, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local outlier factor (LoF)\n",
    "clf = lof(contamination=0.02) # default is 0.1\n",
    "y_pred = clf.fit_predict(X)\n",
    "\n",
    "confusion_matrix(y_pred, ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\lof.py:236: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "# Import the LocalOutlierFactor module\n",
    "from sklearn.neighbors import LocalOutlierFactor as lof\n",
    "\n",
    "# Create the list [1.0, 1.0, ..., 1.0, 10.0] as explained\n",
    "x = [1.0]*30\n",
    "x.append(10.0)\n",
    "\n",
    "# Cast to a data frame\n",
    "X = pd.DataFrame(x)\n",
    "\n",
    "# Fit the local outlier factor and print the outlier scores\n",
    "print(lof().fit_predict(X)) # 1 for normal, -1 for outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoF contamination\n",
    "\n",
    "Your medical advisor at the arrhythmia startup informs you that your training data might not contain all possible types of arrhythmia. How on earth will you detect these other types without any labeled examples? Could an anomaly detector tell the difference between healthy and unhealthy without access to labels? But first, you experiment with the contamination parameter to see its effect on the confusion matrix. You have LocalOutlierFactor as lof, numpy as np, the labels as ground_truth encoded in -1and 1 just like local outlier factor output, and the unlabeled training data as X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamination to match outlier frequency in ground_truth\n",
    "preds = lof(\n",
    "  contamination=np.mean(ground_truth==-1.0)).fit_predict(X)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(confusion_matrix(ground_truth, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Novelty Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novelty LoF\n",
    "\n",
    "# Workaround\n",
    "preds = lof().fit_predict(np.concatenate([X_train, X_test]))\n",
    "preds = preds[X_train.shape[0]:]\n",
    "\n",
    "# Novelty LoF\n",
    "clf = lof(novelty=True)\n",
    "clf.fit(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "clf = LocalOutlierFactor(novelty=True)\n",
    "clf.fit(X_train)\n",
    "y_scores = clf.score_samples(X_test)\n",
    "\n",
    "# One-class Suport Vector Machine\n",
    "clf = OneClassSVM()\n",
    "clf.fit(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "clf = OneClassSVM()\n",
    "clf.fit(X_train)\n",
    "y_scores = clf.score_samples(X_test)\n",
    "threshold = np.quantile(y_scores, 0.1)\n",
    "y_pred = y_scores <= threshold\n",
    "\n",
    "# Isolation Forests\n",
    "clf = IsolationForest()\n",
    "clf.fit(X_train)\n",
    "y_scores = clf.score_samples(X_test)\n",
    "\n",
    "clf = LocalOutlierFactor(novelty=True)\n",
    "clf.fit(X_train)\n",
    "y_scores = clf.score_samples(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing Detectors' Performance ---- > AUC is used \n",
    "\n",
    "clf_lof = LocalOutlierFactor(novelty=True).fit(X_train)\n",
    "clf_isf = IsolationForest().fit(X_train)\n",
    "clf_svm = OneClassSVM().fit(X_train)\n",
    "\n",
    "roc_auc_score(y_test, clf_lof.score_samples(X_test)\n",
    "              \n",
    "roc_auc_score(y_test, clf_isf.score_samples(X_test))\n",
    "              \n",
    "roc_auc_score(y_test, clf_svm.score_samples(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple novelty\n",
    "\n",
    "You find novelty detection more useful than outlier detection, but want to make sure it works on the simple example you came up with before. This time you will use a sequence of thirty examples all with value 1.0 as a training set, and try to see if the example 10.0 is labeled as a novelty. You have access to pandas as pd, and the LocalOutlierFactor module as lof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eturk\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\lof.py:236: FutureWarning: default contamination parameter 0.1 will change in version 0.22 to \"auto\". This will change the predict method behavior.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of thirty 1s and cast to a dataframe\n",
    "X = pd.DataFrame([1.0]*30)\n",
    "\n",
    "# Create an instance of a lof novelty detector\n",
    "detector = lof(novelty=True)\n",
    "\n",
    "# Fit the detector to the data\n",
    "detector.fit(X)\n",
    "\n",
    "# Use it to predict the label of an example with value 10.0\n",
    "print(detector.predict(pd.DataFrame([10.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three novelty detectors\n",
    "\n",
    "Finally, you know enough to run some tests on the use of a few anomaly detectors on the arrhythmia dataset. To test their performance, you will train them on an unlabeled training dataset, but then compare their predictions to the ground truth on the test data using their method .score_samples(). This time, you will be asked to import the detectors as part of the exercise, but you do have the data X_train, X_test, y_train, y_test preloaded as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the novelty detector\n",
    "from sklearn.svm import OneClassSVM as onesvm\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "svm_detector = onesvm().fit(X_train)\n",
    "scores = svm_detector.score_samples(X_test)\n",
    "\n",
    "# Import the novelty detector\n",
    "from sklearn.ensemble import IsolationForest as isof\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "isof_detector = isof().fit(X_train)\n",
    "scores = isof_detector.score_samples(X_test)\n",
    "\n",
    "# Import the novelty detector\n",
    "from sklearn.neighbors import LocalOutlierFactor as lof\n",
    "\n",
    "# Fit it to the training data and score the test data\n",
    "lof_detector = lof(novelty=True).fit(X_train)\n",
    "scores = lof_detector.score_samples(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contamination revisited\n",
    "\n",
    "You notice that one-class SVM does not have a contamination parameter. But you know well by now that you really need a way to control the proportion of examples that are labeled as novelties in order to control your false positive rate. So you decide to experiment with thresholding the scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a one-class SVM detector and score the test data\n",
    "nov_det = onesvm().fit(X_train)\n",
    "scores = nov_det.score_samples(X_test)\n",
    "\n",
    "# Find the observed proportion of outliers in the test data\n",
    "prop = np.mean(y_test==1.0)\n",
    "\n",
    "# Compute the appropriate threshold\n",
    "threshold = np.quantile(scores, prop)\n",
    "\n",
    "# Print the confusion matrix for the thresholded scores\n",
    "print(confusion_matrix(y_test, scores > threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distance-based learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 2.82842712, 5.        ],\n",
       "       [2.82842712, 0.        , 3.60555128],\n",
       "       [5.        , 3.60555128, 0.        ]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distance and similarity\n",
    "from sklearn.neighbors import DistanceMetric as dm\n",
    "dist = dm.get_metric('euclidean')\n",
    "X = [[0,1], [2,3], [0,6]]\n",
    "dist.pairwise(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8284271247461903"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.matrix(X)\n",
    "np.sqrt(np.sum(np.square(X[0,:] - X[1,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Euclidean Local Outlier Factor\n",
    "clf = LocalOutlierFactor(novelty=True, metric='chebyshev')\n",
    "clf.fit(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "dist = dm.get_metric('chebyshev')\n",
    "X = [[0,1], [2,3], [0,6]]\n",
    "dist.pairwise(X)\n",
    "\n",
    "# Hamming Distance Matrix:\n",
    "dist = dm.get_metric('hamming')\n",
    "X = [[0,1], [2,3], [0,6]]\n",
    "dist.pairwise(X)\n",
    "\n",
    "# pdist\n",
    "from scipy.spatial.distance import pdist\n",
    "X = [[0,1], [2,3], [0,6]]\n",
    "pdist(X, 'cityblock')\n",
    "\n",
    "from scipy.spatial.distance import squareform\n",
    "squareform(pdist(X, 'cityblock'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the neighbor\n",
    "\n",
    "It is clear that the local outlier factor algorithm depends a lot on the idea of a nearest neighbor, which in turn depends on the choice of distance metric. So you decide to experiment some more with the hepatitis dataset introduced in the previous lesson. You are given three examples stored in features, whose classes are stored in labels. You will identify the nearest neighbor to the first example (row with index 0) using three different distance metrics, Euclidean, Hamming and Chebyshev, and on the basis of that choose which distance metric to use. You will import the necessary module as part of the exercise, but pandas and numpy already available, as are features and their labels labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DistanceMetric as dm\n",
    "from sklearn.neighbors import DistanceMetric as dm\n",
    "\n",
    "# Find the Euclidean distance between all pairs\n",
    "dist_eucl = dm.get_metric('euclidean').pairwise(features)\n",
    "\n",
    "# Find the Hamming distance between all pairs\n",
    "dist_hamm = dm.get_metric('hamming').pairwise(features)\n",
    "\n",
    "# Find the Chebyshev distance between all pairs\n",
    "dist_cheb = dm.get_metric('chebyshev').pairwise(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all metrics agree\n",
    "\n",
    "In the previous exercise you saw that not all metrics agree when it comes to identifying nearest neighbors. But does this mean they might disagree on outliers, too? You decide to put this to the test. You use the same data as before, but this time feed it into a local outlier factor outlier detector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute outliers according to the Euclidean metric\n",
    "out_eucl = lof(metric='euclidean').fit_predict(features)\n",
    "\n",
    "# Compute outliers according to the Hamming metric\n",
    "out_hamm = lof(metric='hamming').fit_predict(features)\n",
    "\n",
    "# Compute outliers according to the Jaccard metric\n",
    "out_jacc = lof(metric='jaccard').fit_predict(features)\n",
    "\n",
    "# Find if all three metrics agree on any one datapoint\n",
    "print(any(out_jacc + out_hamm + out_eucl == -3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: There is no datapoint that all three metrics flag as an outlier. So choosing a distance metric should be done with great caution! You now have a concrete understanding of the effect of distance metrics on outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unstructured Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stringdist\n",
    "stringdist.levenshtein('abc', 'acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringdist.levenshtein('acc', 'cce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringdist.levenshtein('ILSALVGIV', 'ILSALVGIL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(proteins['sequence'].iloc[:3]).reshape(-1,1)\n",
    "\n",
    "def my_levenshtein(x, y):\n",
    "    return stringdist.levenshtein(x[0], y[0])\n",
    "\n",
    "pdist(sequences, metric=my_levenshtein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein outliers with precomputed matrices\n",
    "\n",
    "# This takes 2 minutes for about 1000 examples\n",
    "M = pdist(sequences, my_levenshtein)\n",
    "\n",
    "# LoF detector with a precomputed distance matrix:\n",
    "# This takes 3 seconds\n",
    "detector = lof(metric='precomputed', contamination=0.1)\n",
    "preds = detector.fit_predict(M)\n",
    "\n",
    "roc_auc_score(proteins['label'] == 'VIRUS', preds == -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restricted Levenshtein\n",
    "\n",
    "You notice that the stringdist package also implements a variation of Levenshtein distance called the Restricted Damerau-Levenshtein distance, and want to try it out. You will follow the logic from the lesson, wrapping it inside a custom function and precomputing the distance matrix before fitting a local outlier factor anomaly detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the RD-Levenshtein metric in a custom function\n",
    "def my_rdlevenshtein(u, v):\n",
    "    return stringdist.rdlevenshtein(u[0], v[0])\n",
    "\n",
    "# Reshape the array into a numpy matrix\n",
    "sequences = np.array(proteins['seq']).reshape(-1, 1)\n",
    "\n",
    "# Compute the pairwise distance matrix in square form\n",
    "M = squareform(pdist(sequences, my_rdlevenshtein))\n",
    "\n",
    "# Run a LoF algorithm on the precomputed distance matrix\n",
    "preds = lof(metric='precomputed').fit_predict(M)\n",
    "\n",
    "# Compute the accuracy of the outlier predictions\n",
    "print(accuracy(proteins['label'] == 'VIRUS', preds == -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing it all together\n",
    "\n",
    "In addition to the distance-based learning anomaly detection pipeline you created in the last exercise, you want to also support a feature-based learning one with one-class SVM. You decide to extract two features: first, the length of the string, and, second, a numerical encoding of the first letter of the string, obtained using the function LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature that contains the length of the string\n",
    "proteins['len'] = proteins['seq'].apply(lambda s: len(s))\n",
    "\n",
    "# Create a feature encoding the first letter of the string\n",
    "proteins['first'] =  LabelEncoder().fit_transform(\n",
    "  proteins['seq'].apply(lambda s: list(s)[0]))\n",
    "\n",
    "# Extract scores from the fitted LoF object, compute its AUC\n",
    "scores_lof = lof_detector.negative_outlier_factor_\n",
    "print(auc(proteins['label']=='IMMUNE SYSTEM', scores_lof))\n",
    "\n",
    "# Fit a 1-class SVM, extract its scores, and compute its AUC\n",
    "svm = OneClassSVM().fit(proteins[['len', 'first']])\n",
    "scores_svm = svm.score_samples(proteins[['len', 'first']])\n",
    "print(auc(proteins['label']=='IMMUNE SYSTEM', scores_svm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
