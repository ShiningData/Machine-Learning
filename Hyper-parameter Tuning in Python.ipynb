{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hyperparameters and Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Introduction & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters in Logistic Regression\n",
    "\n",
    "log_reg_clf = LogisticRegression()\n",
    "log_reg_clf.fit(X_train, y_train)\n",
    "print(log_reg_clf.coef_)\n",
    "\n",
    "# Get the original variable names\n",
    "original_variables = list(X_train.columns)\n",
    "\n",
    "# Zip together the names and coefficients\n",
    "zipped_together = list(zip(original_variables, log_reg_clf.coef_[0]))\n",
    "coefs = [list(x) for x in zipped_together]\n",
    "\n",
    "# Put into a DataFrame with column labels\n",
    "coefs = pd.DataFrame(coefs, columns=[\"Variable\", \"Coefficient\"])\n",
    "\n",
    "# Sort and print top3\n",
    "coefs.sort_values(by=[\"Coefficient\"], axis=0, inplace=True, ascending=False)\n",
    "print(coefs.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters in Random Forest\n",
    "\n",
    "# A simple random forest estimator\n",
    "rf_clf = RandomForestClassifier(max_depth=2)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "# Pull out one tree from the forest\n",
    "chosen_tree = rf_clf.estimators_[7]\n",
    "\n",
    "# Extracting Node Decision\n",
    "\n",
    "# Get the column it split on\n",
    "split_column = chosen_tree.tree_.feature[1]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "\n",
    "# Get the level it split on\n",
    "split_value = chosen_tree.tree_.threshold[1]\n",
    "print(\"This node split on feature {}, at a value of {}\"\n",
    "      .format(split_column_name, split_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting a Logistic Regression parameter\n",
    "\n",
    "# Create a list of original variable names from the training DataFrame\n",
    "original_variables = X_train.columns\n",
    "\n",
    "# Extract the coefficients of the logistic regression estimator\n",
    "model_coefficients = log_reg_clf.coef_[0]\n",
    "\n",
    "# Create a dataframe of the variables and coefficients & print it out\n",
    "coefficient_df = pd.DataFrame({\"Variable\" : original_variables, \"Coefficient\": model_coefficients})\n",
    "print(coefficient_df)\n",
    "\n",
    "# Print out the top 3 positive variables\n",
    "top_three_df = coefficient_df.sort_values(by=\"Coefficient\", axis=0, ascending=False)[0:3]\n",
    "print(top_three_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting a Random Forest parameter\n",
    "\n",
    "# Extract the 7th (index 6) tree from the random forest\n",
    "chosen_tree = rf_clf.estimators_[6]\n",
    "\n",
    "# Visualize the graph using the provided image\n",
    "imgplot = plt.imshow(tree_viz_image)\n",
    "plt.show()\n",
    "\n",
    "# Extract the parameters and level of the top (index 0) node\n",
    "split_column = chosen_tree.tree_.feature[0]\n",
    "split_column_name = X_train.columns[split_column]\n",
    "split_value = chosen_tree.tree_.threshold[0]\n",
    "\n",
    "# Print out the feature and level\n",
    "print(\"This node split on feature {}, at a value of {}\".format(split_column_name, split_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Introduction & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the default hyperparameters for any algorithm\n",
    "rf_clf = RandomForestClassifier()\n",
    "print(rf_clf)\n",
    "\n",
    "log_reg_clf = LogisticRegression()\n",
    "print(log_reg_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring Random Forest Hyperparameters\n",
    "\n",
    "# Print out the old estimator, notice which hyperparameter is badly set\n",
    "print(rf_clf_old)\n",
    "\n",
    "# Get confusion matrix & accuracy for the old rf_model\n",
    "print(\"Confusion Matrix: \\n\\n {} \\n Accuracy Score: \\n\\n {}\".format(\n",
    "  confusion_matrix(y_test, rf_old_predictions),\n",
    "  accuracy_score(y_test, rf_old_predictions))) \n",
    "\n",
    "# Create a new random forest classifier with better hyperparamaters\n",
    "rf_clf_new = RandomForestClassifier(n_estimators=500)\n",
    "\n",
    "# Fit this to the data and obtain predictions\n",
    "rf_new_predictions = rf_clf_new.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Assess the new model (using new predictions!)\n",
    "print(\"Confusion Matrix: \\n\\n\", confusion_matrix(y_test, rf_new_predictions))\n",
    "print(\"Accuracy Score: \\n\\n\", accuracy_score(y_test, rf_new_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters of KNN \n",
    "\n",
    "# Build a knn estimator for each value of n_neighbours\n",
    "knn_5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn_20 = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "# Fit each to the training data & produce predictions\n",
    "knn_5_predictions = knn_5.fit(X_train, y_train).predict(X_test)\n",
    "knn_10_predictions = knn_10.fit(X_train, y_train).predict(X_test)\n",
    "knn_20_predictions = knn_20.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "# Get an accuracy score for each of the models\n",
    "knn_5_accuracy = accuracy_score(y_test, knn_5_predictions)\n",
    "knn_10_accuracy = accuracy_score(y_test, knn_10_predictions)\n",
    "knn_20_accuracy = accuracy_score(y_test, knn_20_predictions)\n",
    "print(\"The accuracy of 5, 10, 20 neighbours was {}, {}, {}\".format(knn_5_accuracy, knn_10_accuracy, knn_20_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Setting & Analyzing Hyperparameter Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automating hyperparameter tuning\n",
    "\n",
    "neighbors_list = [3,5,10,20,50,75]\n",
    "for test_number in neighbors_list:\n",
    "    model = KNeighborsClassifier(n_neighbors=test_number)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_list.append(accuracy)\n",
    "    \n",
    "# Store the results in a dataframe\n",
    "results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy':accuracy_list})\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a learning curve graph\n",
    "\n",
    "neighbors_list = list(range(5,500, 5))\n",
    "accuracy_list = []\n",
    "for test_number in neighbors_list:\n",
    "    model = KNeighborsClassifier(n_neighbors=test_number)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    accuracy_list.append(accuracy)\n",
    "    results_df = pd.DataFrame({'neighbors':neighbors_list, 'accuracy':accuracy_list})\n",
    "\n",
    "plt.plot(results_df['neighbors'], results_df['accuracy'])\n",
    "\n",
    "# Add the labels and title\n",
    "plt.gca().set(xlabel='n_neighbors', ylabel='Accuracy',\n",
    "              title='Accuracy for different n_neighbors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automating Hyperparameter Choice\n",
    "\n",
    "# Set the learning rates & results storage\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "results_list = []\n",
    "\n",
    "# Create the for loop to evaluate model predictions for each learning rate\n",
    "for learning_rate in learning_rates:\n",
    "    model = GradientBoostingClassifier(learning_rate=learning_rate)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    # Save the learning rate and accuracy score\n",
    "    results_list.append([learning_rate, accuracy_score(y_test, predictions)])\n",
    "\n",
    "# Gather everything into a DataFrame\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate', 'accuracy'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Learning Curves\n",
    "\n",
    "# Set the learning rates & accuracies list\n",
    "learn_rates = np.linspace(0.01, 2, num=30)\n",
    "accuracies = []\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rates:\n",
    "    # Create the model, predictions & save the accuracies as before\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, predictions))\n",
    "\n",
    "# Plot results    \n",
    "plt.plot(learn_rates, accuracies)\n",
    "plt.gca().set(xlabel='learning_rate', ylabel='Accuracy', title='Accuracy for different learning_rates')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Introducing Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automating 2 Hyperparameters\n",
    "\n",
    "def gbm_grid_search(learn_rate, max_depth):\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate,max_depth=max_depth)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])\n",
    "\n",
    "results_list = []\n",
    "    for learn_rate in learn_rate_list:\n",
    "        for max_depth in max_depth_list:\n",
    "            results_list.append(gbm_grid_search(learn_rate,max_depth))\n",
    "            \n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate','max_depth','accuracy'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From 2 to N Hyperparameters\n",
    "\n",
    "def gbm_grid_search(learn_rate, max_depth,subsample,max_features):\n",
    "    model = GradientBoostingClassifier(\n",
    "        learning_rate=learn_rate,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        max_features=max_features)\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])\n",
    "\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        for subsample in subsample_list:\n",
    "            for max_features in max_features_list:\n",
    "                results_list.append(gbm_grid_search(learn_rate,max_depth,\n",
    "                                                    subsample,max_features))\n",
    "results_df = pd.DataFrame(results_list, columns=['learning_rate','max_depth','subsample','max_features','accuracy'])\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function\n",
    "def gbm_grid_search(learn_rate, max_depth):\n",
    "\n",
    "    # Create the model\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth)\n",
    "    \n",
    "    # Use the model to make predictions\n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Return the hyperparameters and score\n",
    "    return([learn_rate, max_depth, accuracy_score(y_test, predictions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative tune multiple hyperparameters\n",
    "\n",
    "# Create the relevant lists\n",
    "results_list = []\n",
    "learn_rate_list = [0.01, 0.1, 0.5]\n",
    "max_depth_list = [2,4,6]\n",
    "\n",
    "# Create the for loop\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "        results_list.append(gbm_grid_search(learn_rate,max_depth))\n",
    "\n",
    "# Print the results\n",
    "print(results_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "learn_rate_list = [0.01, 0.1, 0.5]\n",
    "max_depth_list = [2,4,6]\n",
    "\n",
    "# Extend the function input\n",
    "def gbm_grid_search_extended(learn_rate, max_depth, subsample):\n",
    "\n",
    "\t# Extend the model creation section\n",
    "    model = GradientBoostingClassifier(learning_rate=learn_rate, max_depth=max_depth, subsample=subsample)\n",
    "    \n",
    "    predictions = model.fit(X_train, y_train).predict(X_test)\n",
    "    \n",
    "    # Extend the return part\n",
    "    return([learn_rate, max_depth, subsample, accuracy_score(y_test, predictions)])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "# Create the new list to test\n",
    "subsample_list = [0.4, 0.6]\n",
    "\n",
    "for learn_rate in learn_rate_list:\n",
    "    for max_depth in max_depth_list:\n",
    "    \n",
    "        # Extend the for loop\n",
    "        for subsample in subsample_list:\n",
    "        \n",
    "            # Extend the results to include the new hyperparameter\n",
    "            results_list.append(gbm_grid_search_extended(learn_rate, max_depth, subsample))\n",
    "\n",
    "# Print results\n",
    "print(results_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. GridSearch with Sckit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can check all the built in scoring functions this way:\n",
    "from sklearn import metrics\n",
    "sorted(metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cpu count \n",
    "import os\n",
    "print(os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a GridSearchCV object\n",
    "\n",
    "# Create the grid\n",
    "param_grid = {'max_depth': [2, 4, 6, 8],'min_samples_leaf': [1, 2, 4, 6]}\n",
    "\n",
    "#Get a base classifier with some set parameters.\n",
    "rf_class = RandomForestClassifier(criterion='entropy', max_features='auto')\n",
    "\n",
    "grid_rf_class = GridSearchCV(\n",
    "    estimator = rf_class,\n",
    "    param_grid = parameter_grid,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=4,\n",
    "    cv = 10,\n",
    "    refit=True,\n",
    "    return_train_score=True)\n",
    "\n",
    "#Fit the object to our data\n",
    "grid_rf_class.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "grid_rf_class.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier with specified criterion\n",
    "rf_class = RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': [2, 4, 8, 15], 'max_features': ['auto', 'sqrt']}\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_rf_class=GridSearchCV(\n",
    "    estimator=rf_class,\n",
    "    param_grid=param_grid,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=4,\n",
    "    cv=5,\n",
    "    refit=True, return_train_score=True)\n",
    "print(grid_rf_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Understanding a GridSearchCV Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three different groups for the GridSearchCV properties;\n",
    "\n",
    "# 1. A results log\n",
    "    cv_results_\n",
    "    \n",
    "# 2. The best results\n",
    "    best_index_ , best_params_, best_score_\n",
    "\n",
    "# 3. 'Extra information'\n",
    "    scorer_ , n_splits_,  refit_time_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "print(cv_results_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``cv_results_``\n",
    "\n",
    "The `time` columns refer to the time it took to t (and score) the model.\n",
    "\n",
    "The `param_` columns store the parameters it tested on that row, one column per parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The params column contains dictionary of allthe parameters:\n",
    "pd.set_option(\"display.max_colwidth\", -1)\n",
    "print(cv_results_df.loc[:,\"params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `test_score` columns contain the scores on our test set for each of our cross-folds as well as some\n",
    "summary statistics.\n",
    "\n",
    "The `rank_test_score` column, ordering the mean_test_score from best to worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can select the best grid square easily from cv_results_ using the rank_test_score column\n",
    "best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information on the best grid square is neatly summarized in the following three properties:\n",
    "\n",
    "`best_params_`, the dictionary of parameters that gave the best score.\n",
    "\n",
    "`best_score_`, the actual best score.\n",
    "\n",
    "`best_index`, the row in our `cv_results_.rank_test_score` that was the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(grid_rf_class.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_rf_class.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the grid search output\n",
    "\n",
    "# Read the cv_results property into a dataframe & print it out\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "print(cv_results_df)\n",
    "\n",
    "# Extract and print the column with a dictionary of hyperparameters used\n",
    "column = cv_results_df.loc[:, [\"params\"]]\n",
    "print(column)\n",
    "\n",
    "# Extract and print the row that had the best mean test score\n",
    "best_row = cv_results_df[cv_results_df[\"rank_test_score\"] == 1]\n",
    "print(best_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the best results\n",
    "\n",
    "# Print out the ROC_AUC score from the best-performing square\n",
    "best_score = grid_rf_class.best_score_\n",
    "print(best_score)\n",
    "\n",
    "# Create a variable from the row related to the best-performing square\n",
    "cv_results_df = pd.DataFrame(grid_rf_class.cv_results_)\n",
    "best_row = cv_results_df.loc[[grid_rf_class.best_index_]]\n",
    "print(best_row)\n",
    "\n",
    "# Get the n_estimators parameter from the best-performing square and print\n",
    "best_n_estimators = grid_rf_class.best_params_[\"n_estimators\"]\n",
    "print(best_n_estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the best results\n",
    "\n",
    "# See what type of object the best_estimator_ property is\n",
    "print(type(grid_rf_class.best_estimator_))\n",
    "\n",
    "# Create an array of predictions directly using the best_estimator_ property\n",
    "predictions = grid_rf_class.best_estimator_.predict(X_test)\n",
    "\n",
    "# Take a look to confirm it worked, this should be an array of 1's and 0's\n",
    "print(predictions[0:5])\n",
    "\n",
    "# Now create a confusion matrix \n",
    "print(\"Confusion Matrix \\n\", confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Get the ROC-AUC score\n",
    "predictions_proba = grid_rf_class.best_estimator_.predict_proba(X_test)[:,1]\n",
    "print(\"ROC-AUC Score \\n\", roc_auc_score(y_test, predictions_proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Introducing Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a random sample of hyperparameters\n",
    "\n",
    "# Set some hyperparameter lists\n",
    "learn_rate_list = np.linspace(0.001,2,150)\n",
    "min_samples_leaf_list = list(range(1,51))\n",
    "\n",
    "# Create list of combinations\n",
    "from itertools import product\n",
    "combinations_list = [list(x) for x in \n",
    "                     product(learn_rate_list, min_samples_leaf_list)]\n",
    "\n",
    "# Select 100 models from our larger set\n",
    "random_combinations_index = np.random.choice(range(0,len(combinations_random)), \n",
    "                                             100, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Sample Hyperparameters\n",
    "\n",
    "# Create a list of values for the learning_rate hyperparameter\n",
    "learn_rate_list = list(np.linspace(0.01,1.5,200))\n",
    "\n",
    "# Create a list of values for the min_samples_leaf hyperparameter\n",
    "min_samples_list = list(range(10,41))\n",
    "\n",
    "# Combination list\n",
    "combinations_list = [list(x) for x in product(learn_rate_list, min_samples_list)]\n",
    "\n",
    "# Sample hyperparameter combinations for a random search.\n",
    "random_combinations_index = np.random.choice(range(0, len(combinations_list)), 250, replace=False)\n",
    "combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]\n",
    "\n",
    "# Print the result\n",
    "print(combinations_random_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly Search with Random Forest\n",
    "\n",
    "# Create lists for criterion and max_features\n",
    "criterion_list = [\"gini\", \"entropy\"]\n",
    "max_feature_list = [\"auto\", \"sqrt\", \"log2\", None]\n",
    "\n",
    "# Create a list of values for the max_depth hyperparameter\n",
    "max_depth_list = list(range(3,56))\n",
    "\n",
    "# Combination list\n",
    "combinations_list = [list(x) for x in product(criterion_list, max_feature_list, max_depth_list)]\n",
    "\n",
    "# Sample hyperparameter combinations for a random search\n",
    "combinations_random_chosen = random.sample(combinations_list, 150)\n",
    "\n",
    "# Print the result\n",
    "print(combinations_random_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing a Random Search\n",
    "\n",
    "def sample_and_visualize_hyperparameters(n_samples):\n",
    "\n",
    "  # If asking for all combinations, just return the entire list.\n",
    "  if n_samples == len(combinations_list):\n",
    "    combinations_random_chosen = combinations_list\n",
    "  else:\n",
    "    combinations_random_chosen = []\n",
    "    random_combinations_index = np.random.choice(range(0, len(combinations_list)), n_samples, replace=False)\n",
    "    combinations_random_chosen = [combinations_list[x] for x in random_combinations_index]\n",
    "    \n",
    "  # Pull out the X and Y to plot\n",
    "  rand_y, rand_x = [x[0] for x in combinations_random_chosen], [x[1] for x in combinations_random_chosen]\n",
    "\n",
    "  # Plot \n",
    "  plt.clf() \n",
    "  plt.scatter(rand_y, rand_x, c=['blue']*len(combinations_random_chosen))\n",
    "  plt.gca().set(xlabel='learn_rate', ylabel='min_samples_leaf', title='Random Search Hyperparameters')\n",
    "  plt.gca().set_xlim(x_lims)\n",
    "  plt.gca().set_ylim(y_lims)\n",
    "  plt.show()\n",
    "\n",
    "# Confirm how many hyperparameter combinations & print\n",
    "number_combs = len(combinations_list)\n",
    "print(number_combs)\n",
    "\n",
    "# Sample and visualise specified combinations\n",
    "for x in [50, 500, 1500]:\n",
    "    sample_and_visualize_hyperparameters(x)\n",
    "    \n",
    "# Sample all the hyperparameter combinations & visualise\n",
    "sample_and_visualize_hyperparameters(number_combs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Random Search in Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a RandomizedSearchCV Object\n",
    "# we can build a random search object just like the grid search, but with our small change:\n",
    "\n",
    "# Set up the sample space\n",
    "learn_rate_list = np.linspace(0.001,2,150)\n",
    "min_samples_leaf_list = list(range(1,51))\n",
    "\n",
    "# Create the grid\n",
    "parameter_grid = {\n",
    "    'learning_rate' : learn_rate_list,\n",
    "    'min_samples_leaf' : min_samples_leaf_list}\n",
    "\n",
    "# Define how many samples\n",
    "number_models = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random search object\n",
    "random_GBM_class = RandomizedSearchCV(\n",
    "    estimator = GradientBoostingClassifier(),\n",
    "    param_distributions = parameter_grid,\n",
    "    n_iter = number_models,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=4,\n",
    "    cv = 10,\n",
    "    refit=True,\n",
    "    return_train_score = True)\n",
    "\n",
    "# Fit the object to our data\n",
    "random_GBM_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the output\n",
    "\n",
    "rand_x = list(random_GBM_class.cv_results_['param_learning_rate'])\n",
    "rand_y = list(random_GBM_class.cv_results_['param_min_samples_leaf'])\n",
    "\n",
    "# Make sure we set the limits of Y and X appriately\n",
    "x_lims = [np.min(learn_rate_list), np.max(learn_rate_list)]\n",
    "y_lims = [np.min(min_samples_leaf_list), np.max(min_samples_leaf_list)]\n",
    "\n",
    "# Plot grid results\n",
    "plt.scatter(rand_y, rand_x, c=['blue']*10)\n",
    "plt.gca().set(xlabel='learn_rate', ylabel='min_samples_leaf',title='Random Search Hyperparameters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid = {'learning_rate': np.linspace(0.1, 2, 150), 'min_samples_leaf': list(range(20, 65))} \n",
    "\n",
    "# Create a random search object\n",
    "random_GBM_class = RandomizedSearchCV(\n",
    "    estimator = GradientBoostingClassifier(),\n",
    "    param_distributions = param_grid,\n",
    "    n_iter = 10,\n",
    "    scoring='accuracy', n_jobs=4, cv = 5, refit=True, return_train_score = True)\n",
    "\n",
    "# Fit to the training data\n",
    "random_GBM_class.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid\n",
    "param_grid = {'max_depth': list(range(5,26)), 'max_features': ['auto' , 'sqrt']} \n",
    "\n",
    "# Create a random search object\n",
    "random_rf_class = RandomizedSearchCV(\n",
    "    estimator = RandomForestClassifier(n_estimators=80),\n",
    "    param_distributions = param_grid, n_iter = 5,\n",
    "    scoring='roc_auc', n_jobs=4, cv = 3, refit=True, return_train_score = True)\n",
    "\n",
    "# Fit to the training data\n",
    "random_rf_class.fit(X_train, y_train)\n",
    "\n",
    "# Print the values used for both hyperparameters\n",
    "print(random_rf_class.cv_results_['param_max_depth'])\n",
    "print(random_rf_class.cv_results_['param_max_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Comparing Grid and Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample grid coordinates\n",
    "grid_combinations_chosen = combinations_list[0:300]\n",
    "\n",
    "# Print result\n",
    "print(grid_combinations_chosen)\n",
    "\n",
    "# Create a list of sample indexes\n",
    "sample_indexes = list(range(0,len(combinations_list)))\n",
    "\n",
    "# Randomly sample 300 indexes\n",
    "random_indexes = np.random.choice(sample_indexes, 300, replace=False)\n",
    "\n",
    "# Use indexes to create random sample\n",
    "random_combinations_chosen = [combinations_list[index] for index in random_indexes]\n",
    "\n",
    "# Call the function to produce the visualization\n",
    "visualize_search(grid_combinations_chosen, random_combinations_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Informed Search: Coarse to Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Informed Search: Coarse to Fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Coarse to Fine\n",
    "\n",
    "# Confirm the size of the combinations_list\n",
    "print(len(combinations_list))\n",
    "\n",
    "# Sort the results_df by accuracy and print the top 10 rows\n",
    "print(results_df.sort_values(by='accuracy', ascending=False).head(10))\n",
    "\n",
    "# Confirm which hyperparameters were used in this search\n",
    "print(results_df.columns)\n",
    "\n",
    "# Call visualize_hyperparameter() with each hyperparameter in turn\n",
    "visualize_hyperparameter('max_depth')\n",
    "visualize_hyperparameter('min_samples_leaf')\n",
    "visualize_hyperparameter('learn_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coarse to Fine Iterations\n",
    "\n",
    "# Use the provided function to visualize the first results\n",
    "visualize_first()\n",
    "\n",
    "# Create some combinations lists & combine\n",
    "max_depth_list = list(range(1, 21))\n",
    "learn_rate_list = np.linspace(0.001, 1, 50)\n",
    "\n",
    "# Call the function to visualize the second results\n",
    "visualize_second()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see the function definition, you can use Python's handy inspect library, like so:\n",
    "\n",
    "print(inspect.getsource(sample_and_visualize_hyperparameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Informed Methods: Bayesian Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the grid:\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 2, 10, 2),\n",
    "    'min_samples_leaf': hp.quniform('min_samples_leaf', 2, 8, 2),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 1, 55),\n",
    "}\n",
    "\n",
    "# The objective function runs the algorithm:\n",
    "def objective(params):\n",
    "    params = {'max_depth': int(params['max_depth']),\n",
    "              'min_samples_leaf': int(params['min_samples_leaf']),\n",
    "              'learning_rate': params['learning_rate']}\n",
    "    gbm_clf = GradientBoostingClassifier(n_estimators=500,**params)\n",
    "    best_score = cross_val_score(gbm_clf, X_train, y_train,\n",
    "                                 scoring='accuracy', cv=10, n_jobs=4).mean()\n",
    "    loss = 1 - best_score\n",
    "    write_results(best_score, params, iteration)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Run the algorithm\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    max_evals=500,\n",
    "    rstate=np.random.RandomState(42),\n",
    "    algo=tpe.suggest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up space dictionary with specified hyperparameters\n",
    "space = {'max_depth': hp.quniform('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', 0.001, 0.9)}\n",
    "\n",
    "# Set up objective function\n",
    "def objective(params):\n",
    "    params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}\n",
    "    gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) \n",
    "    best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()\n",
    "    loss = 1 - best_score\n",
    "    return loss\n",
    "\n",
    "# Run the algorithm\n",
    "best = fmin(fn=objective,space=space, max_evals=20, rstate=np.random.RandomState(42), algo=tpe.suggest)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Informed Search: Genetic Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key arguments to a TPOT classier are:\n",
    "\n",
    "`generations` – Iterations to run training for.\n",
    "\n",
    "`population_size` – The number of models to keep after each iteration.\n",
    "\n",
    "`offspring_size` – Number of models to produce in each iteration.\n",
    "\n",
    "`mutation_rate` – The proportion of pipelines to apply randomness to.\n",
    "\n",
    "`crossover_rate` – The proportion of pipelines to breed each iteration.\n",
    "\n",
    "`scoring` – The function to determine the best models\n",
    "\n",
    "`cv` – Cross-validation strategy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "\n",
    "tpot = TPOTClassifier(generations=3, population_size=5,\n",
    "                      verbosity=2, offspring_size=10,\n",
    "                      scoring='accuracy', cv=5)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In real life, TPOT is designed to be run for many hours to find the best model. You would have a much larger population and offspring size as well as hundreds more generations to find a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic Hyperparameter Tuning with TPOT\n",
    "\n",
    "# Assign the values outlined to the inputs\n",
    "number_generations = 3\n",
    "population_size = 4\n",
    "offspring_size = 3\n",
    "scoring_function = 'accuracy'\n",
    "\n",
    "# Create the tpot classifier\n",
    "tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size, \n",
    "                          offspring_size=offspring_size, scoring=scoring_function,\n",
    "                          verbosity=2, random_state=2, cv=2)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing TPOT's stability\n",
    "# This assists to see that TPOT is quite unstable when not run for a reasonable amount of time.\n",
    "\n",
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
